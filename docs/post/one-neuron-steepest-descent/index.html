<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <link rel="stylesheet" href="/css/header.css"/>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Arvo"/>
    <title>Deep Learning Concepts Pt. 1: One Neuron Steepest Descent</title>
  </head>
  <body>
    <div class="header">
      <h1><a href="https://aowens.me/">Alex Owens</a></h1>
      <h2>A programming and game development blog.</h2>
    </div>

<head>
  <link rel="stylesheet" href="/css/single.css"/>
</head>
<main>
  <div class="post-title">
    <h1>Deep Learning Concepts Pt. 1: One Neuron Steepest Descent</h1>
    <h4>Posted on February 21, 2018 </h4>
  </div>
  <div class="post-content">
    <hr>
    

<h2 id="one-neuron-steepest-descent-in-python">One Neuron Steepest Descent in Python</h2>

<p>Code is available <a href="https://github.com/aowens-21/basic-learning/blob/master/one-neuron-learning.py">here</a>.</p>

<p>This is going to be a two part series on some very basic introductory concepts of (deep learning)[<a href="https://en.wikipedia.org/wiki/Deep_learning">https://en.wikipedia.org/wiki/Deep_learning</a>], using Python to make a program
that learns the <a href="https://en.wikipedia.org/wiki/Boolean_algebra#Basic_operations">boolean functions</a> AND and OR. We will be using a
specific case of the <a href="https://en.wikipedia.org/wiki/Gradient_descent">steepest descent</a>(also known as gradient descent) to &ldquo;learn the functions&rdquo;,
using <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a> on one layer to compute the weight gradients for our gradient descent.</p>

<h2 id="what-is-a-neuron-in-the-deep-learning-sense">What is a Neuron (in the Deep Learning Sense)?</h2>

<p>In the sense of our deep learning example, the basic structure of a neuron can be broken down into four parts:</p>

<ul>
<li>Inputs</li>
<li>Weights</li>
<li>Bias</li>
<li>Output</li>
</ul>

<p>In this case, we have two inputs because we&rsquo;re trying to learn a boolean function (which operates on two binary values). Let&rsquo;s call our inputs <code>x1</code> and <code>x2</code>.
These two inputs are passed into the neuron and multiplied by corresponding weights: <code>w1</code> and <code>w2</code>. We then add together these two products and add the bias,
which is just another value coming into the neuron. This computation of: <code>x1*w1 + x2*w2 + bias</code> is called the neuron&rsquo;s excitation, which we&rsquo;ll call <code>u</code>. After
<code>u</code> is computed, we pass it into the <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid function</a>, which gives us the actual output of the neuron. These
computations are done in two functions in my example:</p>

<pre><code class="language-python">          def compute_output(weight_vec, input_vec, bias_vec):
              return sigmoid(compute_excitation(weight_vec, input_vec, bias_vec))

          def compute_excitation(weight_vec, input_vec, bias_vec):
              return weight_vec@input_vec + bias_vec
</code></pre>

<p>Just a note, that &lsquo;@&rsquo; symbol means do the dot product of the input and weight vectors (because I&rsquo;m treating all these as vectors). If you&rsquo;re unfamiliar with
the concept of dot products, it basically means multiply the corresponding elements in the two vectors and then sum the products.</p>

<p>We can then compare that output to the output we were expecting, and that&rsquo;s where the learning happens.</p>

<h2 id="steepest-descent-backpropagation-and-other-confusing-words">Steepest Descent, Backpropagation, and other confusing words</h2>

<p>So what exactly is Steepest Descent? Very simply put, steepest descent is an algorithm used in deep learning to find the correct path to
take to reach a given point. We have some goal we want to reach, so we slowly take steps in the direction that most points towards our goal. There&rsquo;s
a really good <a href="https://en.wikipedia.org/wiki/Gradient_descent#An_analogy_for_understanding_gradient_descent">analogy on the Wikipedia page</a> that can
explain the algorithm in high-level terms.</p>

<p>The core, foundational operation for steepest descent is something called the gradient step. To understand this, we first have to understand the concept
of a <a href="https://en.wikipedia.org/wiki/Gradient">gradient</a>. That page is pretty confusing, so I think a more understandable way to think about the gradient is
that it&rsquo;s the way the weights and bias are changed, and in turn how that affects the output of the neuron. This is the <strong>most</strong> important thing to understand
about deep learning. We change the weights and bias based on a computed gradient, and that&rsquo;s exactly how learning happens. We calculate the correct gradient to
apply and eventually we&rsquo;ll get the correct weights to multiply our inputs, thus learning how to interpret the given input set. For those of you (like me) who need
to see code to picture this, here&rsquo;s how the gradient step works in my example:</p>

<p><em>First the gradient computation</em>:</p>

<pre><code class="language-python">          def compute_gradient(output, expected_output, input_vec=1):
              return (output - expected_output) * output * (1 - output) * input_vec
</code></pre>

<p>Don&rsquo;t get too caught up in the actual formula here. It&rsquo;s some semi-complicated calculus derivations that to give us this formula for the correct gradients.
It takes an optional input vector because I wanted to be able to use it to compute the weight gradients and bias gradient with the same formula. The bias
gradient is calculated if we don&rsquo;t provide an input vector.</p>

<p><em>Then later when we apply the gradient step</em>:</p>

<pre><code class="language-python">          weight_gradients = compute_gradient(output, expected_output, input_vec)
          bias_gradient = compute_gradient(output, expected_output)

          # Apply our gradients to weights and bias, with learning rate
          weight_vec -= learning_rate * weight_gradients
          bias_vec -= learning_rate * bias_gradient
</code></pre>

<p>We compute gradients for each part of the neuron and then apply them to our current weights. This code brings up an important concept: the learning rate. As I said
earlier, steepest descent is just taking lots of steps in the direction of our goal value. We use the learning rate, which is some small constant less than 1, to make sure
we don&rsquo;t take too large of a step and end up &ldquo;less learned&rdquo; than we had been. It can be somewhat hard to visualize, but if you think back to the mountain analogy, we don&rsquo;t want
to commit too much to a step downhill because we might get off the quickest path down and ending up more lost than we were in the first place. And since we&rsquo;re computer people, we want
to be off that mountain and at our desk as quickly as possible. The learning rate helps us do that.</p>

<h2 id="loss">Loss</h2>

<p>Perhaps another one of the most important parts of learning is the concept of loss. Loss tells us how far off our mark we were based on the expected output. Generally,
we want to keep learning until our loss is smaller than some goal. This is my python function for computing loss:</p>

<pre><code class="language-python">          def compute_loss(output, expected_output):
              return ((output - expected_output) ** 2) / 2
</code></pre>

<p>This is another formula you don&rsquo;t really need to get hung up on unless you want to go deeper into deep learning. Just understand that it essentially tells us
how well we did this time.</p>

<h2 id="did-it-learn">Did it Learn?</h2>

<p>Let&rsquo;s think back to the original goal of this one neuron example. We want to make the neuron compute AND and OR given two binary inputs. To break this down further,
the program should be able to show the following outputs for the corresponding input pairs.</p>

<table>
<thead>
<tr>
<th>X1</th>
<th>X2</th>
<th>AND</th>
<th>OR</th>
</tr>
</thead>

<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>

<tr>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>

<tr>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>

<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>

<p>In the code, these pairs of inputs and expected outputs are represented like so:</p>

<pre><code class="language-python">          BOOLEAN_AND = ((np.array([0, 0]), 0), (np.array([0, 1]), 0), (np.array([1, 0]), 0), (np.array([1, 1]), 1))
          BOOLEAN_OR = ((np.array([0, 0]), 0), (np.array([0, 1]), 1), (np.array([1, 0]), 1), (np.array([1, 1]), 1))
</code></pre>

<p>The <code>np.array</code> function just constructs a <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html">numpy array</a> (used for dot product and they&rsquo;re more efficient
than Python lists) to hold the input pair. The second item in the tuple is the expected output.</p>

<p>If we run these two lists through my test function:</p>

<pre><code class="language-python">          def output_learning_results(func_learned, weight_vec, bias_vec, iterations):
              for func_pair in func_learned:
                  (input_vec, expected_output) = func_pair
                  output = compute_output(weight_vec, input_vec, bias_vec)
                  print(&quot;Expected: &quot; + str(expected_output) + &quot; Actual: &quot; + str(output))    
              print(&quot;Number of Iterations: &quot; + str(iterations))
</code></pre>

<p>We get the following output:</p>

<pre><code>          // OUTPUT FOR AND
          Expected: 0 Actual: [[  1.41328111e-05]]
          Expected: 0 Actual: [[ 0.02230234]]
          Expected: 0 Actual: [[ 0.02237487]]
          Expected: 1 Actual: [[ 0.97364273]]
          Number of Iterations: 44842

          // OUTPUT FOR OR
          Expected: 0 Actual: [[ 0.03211036]]
          Expected: 1 Actual: [[ 0.97974366]]
          Expected: 1 Actual: [[ 0.97981671]]
          Expected: 1 Actual: [[ 0.99998587]]
          Number of Iterations: 21555
</code></pre>

<p>Looking at this output we can see that while it&rsquo;s not perfectly precise, it clearly worked. It took quite a few iterations because my loss goal was
very, very small, but we can say that to an extent we found the correct weights and bias for the AND and OR functions. I encourage you to go play around with
some of these values, maybe try a different boolean function or higher loss goal. The code is aggressively commented, so you should be able to figure out what
does what.</p>

<h2 id="conclusion">Conclusion</h2>

<p>This is the first part in my deep learning series. In the next post we&rsquo;ll look at a very small network that actually uses two layers to learn the boolean XOR
function, which is impossible for one neuron, or even one layer of neurons to learn. Hopefully this wasn&rsquo;t too dense and you learned something about deep learning, and if
you want to learn more about the ideas behind deep learning, I can recommend <a href="http://www.deeplearningbook.org/">this book</a>. That book is pretty large (it&rsquo;s the textbook for
my college course), however, so if you want something lighter I can also recommend <a href="https://www.amazon.com/Fundamentals-Deep-Learning-Next-Generation-Intelligence/dp/1491925612/ref=sr_1_1?ie=UTF8&amp;qid=1519274871&amp;sr=8-1&amp;keywords=fundamentals+of+deep+learning">this one</a>.</p>

    <hr>
  </div>
  <div class="back-to-index">
    <a href="/post/">Back to Post List</a>
  </div>
</main>
  </body>
</html>

